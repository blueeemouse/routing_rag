### 用LlamaIndex构建naive_rag的问题：

#### 似乎已经解决了。之前报错应该是因为LlamaIndex默认用的是openai的api_url和key，但我用的是第三方的。通过查LlamaIndex文档，发现里面有关于custom_embedding model的[示例](https://developers.llamaindex.ai/python/examples/embeddings/custom_embeddings/)，拿去问了下ai，在naive_rag的实现里(naive_rag_impl.py)中的execute函数里, 加上一段代码

```
from llama_index.embeddings.openai import OpenAIEmbedding
            from llama_index.core import Settings, VectorStoreIndex
            # 1. 创建自定义配置的嵌入模型
            embed_model = OpenAIEmbedding(
                api_key=os.getenv('NAIVE_RAG_API_KEY', os.getenv('GRAPHRAG_API_KEY', 'YOUR_API_KEY_HERE')),
                api_base="https://api.agicto.cn/v1",  # 替换为你的 base URL
                model="text-embedding-ada-002"  # 或你的服务商支持的模型
            )

            # 2. 设置到全局配置（关键步骤）
            Settings.embed_model = embed_model
```

#### 就能解决这个问题了(似乎是, 如果要用第三方的api-url和key, 则需要包装一下)

#### 以及, 对于它回复用的模型, 因为我们让ai生成代码的时候, 它导入的是这段代码 `from llama_index.llms.openai import OpenAI`, 其实就表明只能用openai的模型了. 如果我们想用deepseek的模型, 根据LlamdaIndex的文档, 应该是这样用的:

```
from llama_index.llms.deepseek import DeepSeek


# you can also set DEEPSEEK_API_KEY in your environment variables
llm = DeepSeek(model="deepseek-reasoner", api_key="you_api_key")

# You might also want to set deepseek as your default llm
# from llama_index.core import Settings
# Settings.llm = llm
```

#### (可以看到, 要灵活支持各种模型似乎还挺麻烦?)

